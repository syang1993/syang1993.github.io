<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<title>Demo page for ENHANCING HYBRID SELF-ATTENTION STRUCTURE WITH RELATIVE-POSITION-AWARE BIAS FOR SPEECH SYNTHESIS</title>
</head>
<body>

<h2 align="center"> Audio samples for "ENHANCING HYBRID SELF-ATTENTION STRUCTURE WITH RELATIVE-POSITION-AWARE BIAS FOR SPEECH SYNTHESIS"</h2>
<div><b>Authors:</b> Shan Yang, Heng Lu, Shiying Kang, Lei Xie, Dong Yu</div>
<div><b>Abstract:</b> Recently, compared with the conventional ``front-end"--``back-end"--``vocoder" structure, based on the attention mechanism, end-to-end speech synthesis systems directly train and synthesize from text sequence to the acoustic feature sequence as a whole. More recently, a more calculation efficient architecture named Transformer, which is solely based on self-attention, was proposed to model global dependencies between the input and output sequences. However, although with many advantages, ``Transformer" lacks position information in its structure. And also, the weighted sum form in self-attention may disperse the attention to the whole input sequence other than focusing on the more important neighbouring positions. In order to solve the problems, in this paper, we proposes a hybrid self-attention structure which combines self-attention with the recurrent neural networks (RNNs), and then enhance the proposed structure with relative-position-aware biases. Experiments are conducted to compare proposed hybrid structure with other comparable speech synthesis structures. Mean opinion score (MOS) test results indicate that by enhancing hybrid self-attention structure with relative-position-aware biases, proposed system achieves the best performance with only 0.11 MOS score lower than natural recording.</div>
<div>To BE BUILD</div>  
</body>
</html>
